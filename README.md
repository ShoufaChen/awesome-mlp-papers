# awesome-mlp-papers [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
An up-to-date list of MLP-based paper without attention! Maintained by [Haofan Wang](https://haofanwang.github.io/) (haofanwang.ai@gmail.com).


## MLP is all you Need

[Less is More: Pay Less Attention in Vision Transformers](https://arxiv.org/abs/2105.14217), Monash University, 2021

[Can Attention Enable MLPs To Catch Up With CNNs?](https://arxiv.org/abs/2105.15078), Tsinghua University, CVM 2021

[Pay Attention to MLPs](https://arxiv.org/abs/2105.08050), Google Research, 2021, [[code](https://github.com/jaketae/g-mlp)]

[FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824), Google Research, 2021, [[code](https://github.com/rishikksh20/FNet-pytorch)]

[ResMLP: Feedforward networks for image classification with data-efficient training](https://arxiv.org/abs/2105.03404), Facebook AI, CVPR 2021, [[code]](https://github.com/lucidrains/res-mlp-pytorch)

[Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://arxiv.org/abs/2105.03322), Google Research, ACL 2021

[Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet](https://arxiv.org/abs/2105.02723), Oxford University, 2021, [[code]](https://github.com/lukemelas/do-you-even-need-attention)

[RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition](https://arxiv.org/abs/2105.01883), Tsinghua University, 2021, [[code]](https://github.com/DingXiaoH/RepMLP)

[Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks](https://arxiv.org/abs/2105.02358), Tsinghua University, 2021

[MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601), Google Research, 2021, [[code]](https://github.com/lucidrains/mlp-mixer-pytorch)

[Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743), Google Research, ICML 2021


## Contributing
Please help in contributing to this list by submitting an [issue](https://github.com/haofanwang/awesome-mlp-papers/issues) or a [pull request](https://github.com/haofanwang/awesome-mlp-papers/pulls)

```markdown
- Paper Name [[pdf]](link) [[code]](link)
```

## Other topics
More interested advanced resources can be found at https://github.com/haofanwang/Awesome-Computer-Vision
